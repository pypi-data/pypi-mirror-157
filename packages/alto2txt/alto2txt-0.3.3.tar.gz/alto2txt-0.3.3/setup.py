# -*- coding: utf-8 -*-
from setuptools import setup

package_dir = \
{'': 'src'}

packages = \
['alto2txt', 'alto2txt.xslts']

package_data = \
{'': ['*']}

install_requires = \
['lxml>=4.7.1,<5.0.0']

setup_kwargs = {
    'name': 'alto2txt',
    'version': '0.3.3',
    'description': 'extract plain text and minimal metadata from ALTO xml files',
    'long_description': '# Extract plain text from newspapers (alto2txt 0.3.1)\n\nConverts XML (in METS 1.8/ALTO 1.4, METS 1.3/ALTO 1.4, BLN or UKP format) publications to plaintext articles and generates minimal metadata. \n\n\n## [Full documentation and demo instructions.](https://living-with-machines.github.io/alto2txt/#/)\n\n\n## Installation\n\n### Installation using an Anaconda environment\n\nWe recommend installation via Anaconda:\n\n* Refer to the [Anaconda website and follow the instructions](https://docs.anaconda.com/anaconda/install/).\n\n* Create a new environment for alto2txt\n\n```bash\nconda create -n py37alto python=3.7\n```\n\n* Activate the environment:\n\n```bash\nconda activate py37alto\n```\n\n* Install alto2txt itself\n\nInstall `alto2txt` using pip:\n\n```bash\npip install alto2txt\n```\n\n(For now it is still necessary to install using pip. In due course we plan to make alto2txt available through a conda channel, meaning that it can be installed directly using conda commands.)\n\n### Installation using pip, outside an Anaconda environment\n\nNote, the use of `alto2txt`` outside a conda environment has not been as extensively tested as within a conda environment. Whilst we believe that this should work, please use with caution.\n\n```bash\npip install alto2txt\n```\n\n### Installation of a test release\n\nIf you need (or want) to install a test release of `alto2txt` you will likely be advised of the specific version number to install. This examaple command will install `v0.3.1-alpha.20`:\n\n```bash\npip install -i https://test.pypi.org/simple/ alto2txt==0.3.1a20\n```\n\n## Usage\n\nDownsampling can be used to convert only every Nth issue of each newspaper. One text file is output per article, each complemented by one XML metadata file.\n\n\n\n```\nextract_publications_text.py [-h] [-d [DOWNSAMPLE]]\n                                    [-p [PROCESS_TYPE]]\n                                    [-l [LOG_FILE]]\n                                    [-n [NUM_CORES]]\n                                    xml_in_dir txt_out_dir\n\nConverts XML publications to plaintext articles\n\npositional arguments:\n  xml_in_dir            Input directory with XML publications\n  txt_out_dir           Output directory for plaintext articles\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -d [DOWNSAMPLE], --downsample [DOWNSAMPLE]\n                        Downsample. Default 1\n  -l [LOG_FILE], --log-file [LOG_FILE]\n                        Log file. Default out.log\n  -p [PROCESS_TYPE], --process-type [PROCESS_TYPE]\n                        Process type.\n                        One of: single,serial,multi,spark\n                        Default: multi\n  -n [NUM_CORES], --num-cores [NUM_CORES]\n                        Number of cores (Spark only). Default 1")\n```\n\n`xml_in_dir` is expected to hold XML for multiple publications, in the following structure:\n\n```\nxml_in_dir\n|-- publication\n|   |-- year\n|   |   |-- issue\n|   |   |   |-- xml_content\n|   |-- year\n|-- publication\n```\n\nHowever, if `-p|--process-type single` is provided then `xml_in_dir` is expected to hold XML for a single publication, in the following structure:\n\n```\nxml_in_dir\n|-- year\n|   |-- issue\n|   |   |-- xml_content\n|-- year\n```\n\n`txt_out_dir` is created with an analogous structure to `xml_in_dir`.\n\n`PROCESS_TYPE` can be one of:\n\n* `single`: Process single publication.\n* `serial`: Process publications serially.\n* `multi`: Process publications using multiprocessing (default).\n* `spark`: Process publications using Spark.\n\n`DOWNSAMPLE` must be a positive integer, default 1.\n\nThe following XSLT files need to be in an `extract_text.xslts` module:\n\n* `extract_text_mets18.xslt`: METS 1.8 XSL file.\n* `extract_text_mets13.xslt`: METS 1.3 XSL file.\n* `extract_text_bln.xslt`: BLN XSL file.\n* `extract_text_ukp.xslt`: UKP XSL file.\n\n## Process publications\n\nAssume `~/BNA` exists and matches the structure above.\n\nExtract text from every publication:\n\n```bash\n./extract_publications_text.py ~/BNA txt\n```\n\nExtract text from every 100th issue of every publication:\n\n```bash\n./extract_publications_text.py ~/BNA txt -d 100\n```\n\n## Process a single publication\n\nExtract text from every issue of a single publication:\n\n```bash\n./extract_publications_text.py -p single ~/BNA/0000151 txt\n```\n\nExtract text from every 100th issue of a single publication:\n\n```bash\n./extract_publications_text.py -p single ~/BNA/0000151 txt -d 100\n```\n\n## Configure logging\n\nBy default, logs are put in `out.log`.\n\nTo specify an alternative location for logs, use the `-l` flag e.g.\n\n```bash\n./extract_publications_text.py -l mylog.txt ~/BNA txt -d 100 2> err.log\n```\n\n## Process publications via Spark\n\n[Information on running on spark.](spark_instructions.md)\n\n\n## Future work\n\nFor a complete list of future plans see the [GitHub issues list](https://github.com/Living-with-machines/alto2txt/issues). Some highlights include:\n\n* Export more metadata from alto, probably by parsing `mets` first.\n* Check and ensure that articles that span multiple pages are pulled into a single article file.\n* Smarter handling of articles spanning multiple pages.\n\n# Copyright\n\n## Software\n\nCopyright 2022 The Alan Turing Institute, British Library Board, Queen Mary University of London, University of Exeter, University of East Anglia and University of Cambridge.\n\nSee [LICENSE](LICENSE) for more details.\n\n## Example Datasets\n\nThis repo contains example datasets, which have been taken from the [British Library Research Repository](https://bl.iro.bl.uk/concern/datasets/551cdd7b-580d-472d-8efb-b7f05cf64a11) ([DOI link](https://doi.org/10.23636/1235)).\n\nThis data is "CC0 1.0 Universal Public Domain" - [No Copyright - Other Known Legal Restrictions](https://rightsstatements.org/page/NoC-OKLR/1.0/?language=en)\n\n- There is a subset of the example data in the `demo-files` directory.\n- There are adapted copies of the data in the `tests/tests/test_files` directory. These have been edited to test errors and edge cases.\n\n# Funding and Acknowledgements\n\nThis software has been developed as part of the [Living with Machines](https://livingwithmachines.ac.uk) project.\n\nThis project, funded by the UK Research and Innovation (UKRI) Strategic Priority Fund, is a multidisciplinary collaboration delivered by the Arts and Humanities Research Council (AHRC), with The Alan Turing Institute, the British Library and the Universities of Cambridge, East Anglia, Exeter, and Queen Mary University of London.\n',
    'author': 'Mike Jackson',
    'author_email': 'mjackson@turing.ac.uk',
    'maintainer': None,
    'maintainer_email': None,
    'url': None,
    'package_dir': package_dir,
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'python_requires': '>3.7.0',
}


setup(**setup_kwargs)
